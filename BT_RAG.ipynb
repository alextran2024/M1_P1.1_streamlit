{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "724e70ceb031479d8c37b49868d9f8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e8915e1315344aabeba33240acc097f",
              "IPY_MODEL_31f571b52b9144a5a21b1f64eef482e4",
              "IPY_MODEL_afb00992b8f04c499490253cd7cb0a39"
            ],
            "layout": "IPY_MODEL_bebf0cb48fac4dada89b10dfda03d306"
          }
        },
        "9e8915e1315344aabeba33240acc097f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd1d4372eea4a02a75431359a7ed6f7",
            "placeholder": "​",
            "style": "IPY_MODEL_97f2a64859de4ffa9da6be87c5419913",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "31f571b52b9144a5a21b1f64eef482e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a3594931d614fab9ef0f79b1ffba318",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8a2a957b6b940789d63dd155bd6f5b9",
            "value": 2
          }
        },
        "afb00992b8f04c499490253cd7cb0a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71ec0f5cf74940aea617226ea978db4a",
            "placeholder": "​",
            "style": "IPY_MODEL_5c03a5b1d6df4f0b91ea5ed0f36a658e",
            "value": " 2/2 [01:16&lt;00:00, 34.93s/it]"
          }
        },
        "bebf0cb48fac4dada89b10dfda03d306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fd1d4372eea4a02a75431359a7ed6f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f2a64859de4ffa9da6be87c5419913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a3594931d614fab9ef0f79b1ffba318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8a2a957b6b940789d63dd155bd6f5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71ec0f5cf74940aea617226ea978db4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c03a5b1d6df4f0b91ea5ed0f36a658e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alextran2024/M1_P1.1_streamlit/blob/main/BT_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "393w60_rgs5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b8db4d-48f2-40f8-897c-e7983540d3d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.41.2\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q accelerate==0.31.0\n",
        "!pip install -q langchain==0.2.5\n",
        "!pip install -q langchainhub==0.1.20\n",
        "!pip install -q langchain-chroma==0.1.1\n",
        "!pip install -q langchain-community==0.2.5\n",
        "!pip install -q langchain_huggingface==0.0.3\n",
        "!pip install -q python-dotenv==1.0.1\n",
        "!pip install -q pypdf==4.2.0\n",
        "!pip install -q numpy==1.25.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tk3GGDkWfvgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Loader = PyPDFLoader\n",
        "FILE_PATH = \"https://drive.google.com/file/d/1lWuq0COKnU9mCfMvTEq54DBLgAh3yYDx\"\n",
        "loader = Loader(FILE_PATH)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter (chunk_size=1000, chunk_overlap=100)\"\"\"\n",
        "\n",
        "# URL of the PDF file\n",
        "FILE_URL = \"https://drive.google.com/uc?export=download&id=1lWuq0COKnU9mCfMvTEq54DBLgAh3yYDx\"\n",
        "LOCAL_FILE_PATH = \"downloaded_file.pdf\"\n",
        "\n",
        "# Download the PDF file\n",
        "response = requests.get(FILE_URL)\n",
        "with open(LOCAL_FILE_PATH, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "try:\n",
        "    # Load the PDF file\n",
        "    loader = PyPDFLoader(LOCAL_FILE_PATH)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split the documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Print out the chunks or process them further\n",
        "    for chunk in chunks:\n",
        "        print(chunk)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading PDF: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJBBG8A-xRz3",
        "outputId": "20be0f11-14b6-487b-c043-ef080a90edf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='AI VIET NAM – AI COURSE 2024\n",
            "Tutorial: Phát hiện đối tượng trong ảnh với\n",
            "YOLOv10\n",
            "Dinh-Thang Duong, Nguyen-Thuan Duong, Minh-Duc Bui và\n",
            "Quang-Vinh Dinh\n",
            "Ngày 20 tháng 6 năm 2024\n",
            "I. Giới thiệu\n",
            "Object Detection (Tạm dịch: Phát hiện đối tượng) là một bài toán cổ điển thuộc lĩnh vực\n",
            "Computer Vision. Mục tiêu của bài toán này là tự động xác định vị trí của các đối tượng trong\n",
            "một tấm ảnh. Tính tới thời điểm hiện tại, đã có rất nhiều phương pháp được phát triển nhằm\n",
            "giải quyết hiệu quả bài toán này. Trong đó, các phương pháp thuộc họ YOLO (You Only Look\n",
            "Once) thu hút được sự chú ý rất lớn từ cộng đồng nghiên cứu bởi độ chính xác và tốc độ thực\n",
            "thi mà loại mô hình này mang lại.\n",
            "Hình 1: Logo của mô hình YOLO. Ảnh: link.\n",
            "Thời gian vừa qua, Ao Wang và các cộng sự tại Đại học Thanh Hoa (Tsinghua University)\n",
            "đã đề xuất mô hình YOLOv10 trong bài báo YOLOv10: Real-Time End-to-End Object\n",
            "Detection [10]. Với những cải tiến mới, mô hình đã đạt được hiệu suất vượt trội hơn so với các' metadata={'source': 'downloaded_file.pdf', 'page': 0}\n",
            "page_content='Detection [10]. Với những cải tiến mới, mô hình đã đạt được hiệu suất vượt trội hơn so với các\n",
            "phiên bản YOLO trước đó ở các khía cạnh khác nhau, tăng cường khả năng phát hiện đối tượng\n",
            "theo thời gian thực (real-time object detection).\n",
            "1' metadata={'source': 'downloaded_file.pdf', 'page': 0}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Hình 2: Hiệu suất của YOLOv10 khi so sánh với các mô hình khác. Trên tập dữ liệu COCO,\n",
            "YOLOv10 đạt được kết quả tốt nhất về khía cạnh Độ trễ (Latency) và Số lượng tham số mô\n",
            "hình (Number of parameters) trong khi vẫn giữ được độ chính xác (COCO AP) cao. Ảnh: [10].\n",
            "Trong bài viết này, chúng ta sẽ cùng nhau tìm hiểu về YOLOv10 và cách sử dụng mô hình này.\n",
            "Thông qua đó, nhóm cũng sẽ trình bày sơ lược về bài toán Object Detection cũng như tóm tắt\n",
            "ngắn gọn các phiên bản YOLO trước đó để bạn đọc có một cái nhìn tổng quan hơn về nội dung\n",
            "này.\n",
            "Theo đó, bài viết được bố cục như sau:\n",
            "-Phần I: Giới thiệu về nội dung bài viết.\n",
            "-Phần II: Tóm tắt về bài toán Object Detection và các phiên bản YOLO đời trước.\n",
            "-Phần III: Trình bày nội dung YOLOv10.\n",
            "-Phần IV: Hướng dẫn cách cài đặt, huấn luyện và sử dụng YOLOv10.\n",
            "-Phần V: Trích dẫn tài liệu.\n",
            "2' metadata={'source': 'downloaded_file.pdf', 'page': 1}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "II. Bài toán Object Detection và các\n",
            "phiên bản YOLO đời trước\n",
            "II.I. Bài toán Object Detection\n",
            "Trong Computer Vision, bài toán Object Detection hướng đến xây dựng một chương trình có\n",
            "thể tự động xác định vị trí và nhận diện tên (class) của các vật thể trong một bức ảnh. Tổng\n",
            "hợp hai thông tin đầu ra này còn được gọi với tên là bounding box. Từ đây, ta có thể mô tả\n",
            "Input/Output của một chương trình Object Detection như sau:\n",
            "-Input:Một bức ảnh.\n",
            "-Output: Bounding box của các vật thể cần phát hiện trong ảnh.\n",
            "Hình 3: Minh họa Input/Output của bài toán Object Detection.\n",
            "Đến thời điểm hiện tại, các phương pháp sử dụng mạng Deep Learning cho thấy hiệu suất vượt\n",
            "trội. Ta có thể tóm tắt các hướng tiếp cận theo ba dạng như sau:\n",
            "1.One-stage Object Detection: Việc xác định vị trí tọa độ và phân loại tên class của các\n",
            "vật thể được thực hiện trên một bước duy nhất. Điển hình cho hướng tiếp cận này có thể\n",
            "kể đến SSD [13] và YOLO [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].' metadata={'source': 'downloaded_file.pdf', 'page': 2}\n",
            "page_content='kể đến SSD [13] và YOLO [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].\n",
            "2.Two-stage Object Detection: Việc xác định vị trí tọa độ và phân loại tên class của các\n",
            "vật thể được thực hiện riêng biệt. Điển hình cho hướng tiếp cận này có thể kể đến RCNN\n",
            "[14] và Faster RCNN [15].\n",
            "3.End-to-end Object Detection: Việc xác định vị trí tọa độ và phân loại tên class của\n",
            "các vật thể được dự đoán bởi một mô hình duy nhất (không sử dụng các bước tiền và hậu\n",
            "xử lý bounding box). Điển hình cho hướng tiếp cận này có thể kể đến DETR [16], DINO\n",
            "[17], và DeFCN [18].\n",
            "3' metadata={'source': 'downloaded_file.pdf', 'page': 2}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Ở phần sau, chúng ta sẽ tập trung điểm qua các phiên bản YOLO (từ v1 đến v9).\n",
            "II.II. YOLOv1\n",
            "YOLOv1 [1] là mô hình one-stage (hoặc single-stage) real-time object detection được giới thiệu\n",
            "vào năm 2016.\n",
            "Hình 4: Kiến trúc mô hình YOLOv1 với 24 lớp conv và 2 lớp mlp. Ảnh: [1].\n",
            "-Điểm mới : YOLOv1 sử dụng một mạng neural đơn để dự đoán cả vị trí và tên class của\n",
            "các object trực tiếp từ ảnh đầu vào.\n",
            "-Ưu điểm : Tốc độ nhanh, khả năng object detection theo thời gian thực.\n",
            "-Nhược điểm : Độ chính xác không cao với các object nhỏ hoặc bị che khuất.\n",
            "II.III. YOLOv2\n",
            "YOLOv2 [2], còn được gọi là YOLO9000, được giới thiệu vào năm 2017 với nhiều cải tiến so với\n",
            "YOLOv1.\n",
            "Hình 5: Hình ảnh minh họa về anchor boxes. Ảnh: Zixuan Zhang.\n",
            "4' metadata={'source': 'downloaded_file.pdf', 'page': 3}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "-Điểm mới : Sử dụng anchor boxes, mạng Darknet-19, và tăng training data để tăng độ\n",
            "chính xác.\n",
            "-Ưu điểm : Tăng độ chính xác và khả năng nhận diện nhiều object trong 1 cell.\n",
            "-Nhược điểm : Phức tạp hơn, cần nhiều tài nguyên tính toán, và khó detect các object\n",
            "nhỏ.\n",
            "II.IV. YOLOv3\n",
            "YOLOv3 [3] ra mắt năm 2018, tiếp tục cải tiến từ YOLOv2.\n",
            "Hình 6: Kiến trúc mô hình YOLOv3. Ảnh: [3].\n",
            "-Điểm mới : Sử dụng mạng Darknet-53 và detect object ở ba cấp độ khác nhau (multi-scale\n",
            "detection) để cải thiện độ chính xác.\n",
            "-Ưu điểm : Độ chính xác cao hơn, khả năng phát hiện object nhỏ tốt hơn.\n",
            "-Nhược điểm : Tốc độ chậm hơn so với các phiên bản trước do sự phức tạp của mô hình.\n",
            "II.V. YOLOv4\n",
            "YOLOv4 [4] ra mắt năm 2020, với mục tiêu cải thiện cả độ chính xác và tốc độ.\n",
            "-Điểm mới : Sử dụng nhiều kỹ thuật mới như CSPDarknet53, PANet, và nhiều cải tiến\n",
            "khác.\n",
            "-Ưu điểm : Cân bằng tốt giữa tốc độ và độ chính xác, dễ dàng sử dụng và triển khai.' metadata={'source': 'downloaded_file.pdf', 'page': 4}\n",
            "page_content='khác.\n",
            "-Ưu điểm : Cân bằng tốt giữa tốc độ và độ chính xác, dễ dàng sử dụng và triển khai.\n",
            "-Nhược điểm : Yêu cầu phần cứng mạnh để đạt hiệu năng tối ưu.\n",
            "5' metadata={'source': 'downloaded_file.pdf', 'page': 4}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "II.VI. YOLOv5\n",
            "YOLOv5 [5], không phải do tác giả gốc phát triển, nhưng được cộng đồng sử dụng rộng rãi từ\n",
            "năm 2020.\n",
            "-Điểm mới : Tập trung vào tối ưu hóa và dễ dàng sử dụng với các framework như PyTorch.\n",
            "Sử dụng CSPNet làm backbone và PANet để fusion giúp cải thiện độ chính xác của mô\n",
            "hình.\n",
            "-Ưu điểm : Dễ dàng triển khai, tối ưu hóa tốt, cộng đồng hỗ trợ mạnh mẽ.\n",
            "-Nhược điểm : Yêu cầu tài nguyên tính toán cao và khó detect được các object nhỏ.\n",
            "II.VII. YOLOv6\n",
            "YOLOv6 [6] là phiên bản tiếp theo với nhiều cải tiến về tốc độ và độ chính xác.\n",
            "-Điểm mới : Sử dụng backbone mới EfficientRep và Rep-PAN Neck để tối ưu hóa và tăng\n",
            "hiệu năng của mô hình. SimOTA, một phương pháp Label Assignment, cũng được sử dụng\n",
            "để tăng tính ổn định khi training.\n",
            "-Ưu điểm : Hiệu năng cao hơn, tốc độ nhanh hơn.\n",
            "-Nhược điểm : Yêu cầu tài nguyên tính toán cao hơn.\n",
            "II.VIII. YOLOv7\n",
            "YOLOv7 [7] tiếp tục phát triển với các cải tiến về mô hình và thuật toán.' metadata={'source': 'downloaded_file.pdf', 'page': 5}\n",
            "page_content='II.VIII. YOLOv7\n",
            "YOLOv7 [7] tiếp tục phát triển với các cải tiến về mô hình và thuật toán.\n",
            "-Điểm mới : Sử dụng backbone E-ELAN kết hợp với phương pháp trainable bag-of-freebies\n",
            "để tăng độ chính xác của mô hình mà không làm tăng chi phí tính toán.\n",
            "-Ưu điểm : Tăng độ chính xác và khả năng nhận diện trong các điều kiện phức tạp.\n",
            "-Nhược điểm : Mức độ phức tạp cao, cần nhiều thời gian và tài nguyên để huấn luyện.\n",
            "II.IX. YOLOv8\n",
            "YOLOv8 [8] được giới thiệu vào năm 2023 bởi Ultralytics. Mô hình này cải thiện độ chính xác\n",
            "và tốc độ so với YOLOv7 và giới thiệu nhiều tính năng mới như anchor-free detection.\n",
            "6' metadata={'source': 'downloaded_file.pdf', 'page': 5}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Hình 7: So sánh 2 phương pháp hand-crafted anchor (trên) và anchor-free (dưới). Ảnh: [11].\n",
            "-Điểm mới : Sử dụng anchor-free detection, giúp đơn giản hóa kiến trúc mô hình và cải\n",
            "thiện hiệu suất.\n",
            "-Ưu điểm :\n",
            "+ Độ chính xác cao hơn: YOLOv8 đạt mAP 50.2% trên bộ dữ liệu COCO, cao hơn so\n",
            "với YOLOv7.\n",
            "+ Dễ sử dụng: YOLOv8 có giao diện Python và CLI dễ sử dụng, giúp người dùng dễ\n",
            "dàng triển khai và huấn luyện mô hình.\n",
            "-Nhược điểm : Yêu cầu tài nguyên tính toán cao: Mặc dù có nhiều cải tiến, YOLOv8 vẫn\n",
            "yêu cầu nhiều tài nguyên tính toán, đặc biệt là khi xử lý hình ảnh độ phân giải cao.\n",
            "II.X. YOLOv9\n",
            "YOLOv9 [9] được giới thiệu vào năm 2024 bởi Chien-Yao Wang, I-Hau Yeh, và Hong-Yuan Mark\n",
            "Liao. Mô hình này cải thiện độ chính xác và tốc độ so với YOLOv8 và giới thiệu nhiều kỹ thuật\n",
            "mới như Programmable Gradient Information (PGI) và Generalized Efficient Layer Aggregation\n",
            "Network (GELAN).\n",
            "7' metadata={'source': 'downloaded_file.pdf', 'page': 6}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Hình 8: PGI và các kiến trúc tương tự. Ảnh: [9].\n",
            "-Điểm mới : YOLOv9 sử dụng PGI và GELAN để cải thiện độ chính xác và hiệu suất của\n",
            "mô hình.\n",
            "-Ưu điểm :\n",
            "+ Kiến trúc tiên tiến: Sử dụng PGI và GELAN giúp mô hình duy trì thông tin quan\n",
            "trọng và tối ưu hóa quá trình huấn luyện, làm cho YOLOv9 trở nên mạnh mẽ và linh\n",
            "hoạt hơn trong nhiều ứng dụng khác nhau.\n",
            "+ Tốc độ nhanh hơn: YOLOv9 có thể xử lý hình ảnh nhanh hơn so với YOLOv8 nhờ\n",
            "vào các cải tiến trong kiến trúc mạng.\n",
            "-Nhược điểm :\n",
            "+ Mặc dù nhanh hơn YOLOv8, YOLOv9 vẫn yêu cầu nhiều tài nguyên tính toán, đặc\n",
            "biệt là khi xử lý hình ảnh độ phân giải cao.\n",
            "+ Mặc dù cải thiện so với YOLOv8, YOLOv9 vẫn gặp khó khăn trong việc phát hiện\n",
            "các object rất nhỏ.\n",
            "Hình 9: GELAN và các kiến trúc tương tự. Ảnh: [9].\n",
            "8' metadata={'source': 'downloaded_file.pdf', 'page': 7}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "III. YOLOv10: Real-Time End-to-End\n",
            "Object Detection\n",
            "Ao Wang và các cộng sự đã đặt nghi vấn về sự tối ưu trong việc phụ thuộc vào kỹ thuật hậu xử\n",
            "lý Non-maximum Suppresion (NMS) và cách thiết kế mô hình của các phiên bản YOLO trước\n",
            "đó. Với các hạn chế quan sát được từ hai điều trên và mục tiêu xây dựng một mô hình object\n",
            "detection thời gian thực, YOLOv10 đã được đề xuất với những thay đổi mới. Theo đó, có hai\n",
            "điểm nhấn chính trong phương pháp mà nhóm tác giả YOLOv10 đề xuất bao gồm:\n",
            "1.Consistent Dual Assignments for NMS-free Training: Trong quá trình dự đoán của\n",
            "các mạng YOLO đời trước, rất nhiều bounding box được mô hình đưa ra (ví dụ: anchors\n",
            "box,...) và nhiệm vụ của chúng ta là tìm ra đại diện chính xác nhất cho mỗi vật thể có\n",
            "trong ảnh. Để tận dụng tối đa các đề xuất bounding box đúng trong việc huấn luyện,\n",
            "các phương pháp thường ứng dụng kỹ thuật Task Alignment Learning (TAL). Trong đó,' metadata={'source': 'downloaded_file.pdf', 'page': 8}\n",
            "page_content='các phương pháp thường ứng dụng kỹ thuật Task Alignment Learning (TAL). Trong đó,\n",
            "chiến lược one-to-many label assignment được áp dụng để gán các bounding box “positive”\n",
            "(bounding box chính xác) vào ground-truth của vật thể tương ứng để tăng cường khả năng\n",
            "nhận biết vật thể của mô hình. Tuy vậy, việc này lại gây ra độ trễ (latency) lớn trong quá\n",
            "trình inference của mô hình bởi việc phụ thuộc vào thuật toán NMS để lọc các dự đoán\n",
            "thừa.\n",
            "Một cách tiếp cận khác đó là sử dụng chiến lược one-to-one label assignment, bằng cách\n",
            "chỉ gán một đề xuất bouding box “positive” với ground-truth của vật thể tương ứng, qua\n",
            "đó tránh việc hậu xử lý với NMS. Tuy vậy, chiến lược này lại dẫn đến hiệu suất mô hình\n",
            "không được tốt.\n",
            "Hình 10: Minh họa chiến lược one-to-one và one-to-many label assignemnts.\n",
            "9' metadata={'source': 'downloaded_file.pdf', 'page': 8}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Để khắc phục trình trạng của hai cách nêu trên, YOLOv10 cài đặt một chiến lược huấn\n",
            "luyện mới là sự kết hợp của one-to-one và one-to-many, mang tên Dual label assignments.\n",
            "Chiến lược này được minh họa theo như hình sau:\n",
            "Hình 11: Minh họa chiến lược Dual label assignments. Ảnh: [10].\n",
            "Về cơ bản, trong quá trình huấn luyện, tác giả sử dụng thông tin của cả hai chiến lược.\n",
            "Đến quá trình inference, nhánh one-to-many sẽ được bỏ đi để tránh việc sử dụng NMS. Về\n",
            "cách bắt cặp ground-truth và bounding box dự đoán, cả hai chiến lược đều sử dụng chung\n",
            "một độ đo là Consistent Matching Metric.\n",
            "2.Holistic Efficiency-Accuracy Driven Model Design: Bên cạnh kỹ thuật huấn luyện,\n",
            "việc thiết kế kiến trúc mô hình cho YOLO vẫn còn đó những thách thức và hạn chế để khắc\n",
            "phục theo tiêu chí độ hiệu quả (efficiency) và độ chính xác (accuracy). Về độ hiệu quả, dựa\n",
            "trên kiến trúc YOLO của bản trước (YOLOv8), nhóm tác giả thực hiện hiệu chỉnh các nội\n",
            "dung sau:' metadata={'source': 'downloaded_file.pdf', 'page': 9}\n",
            "page_content='trên kiến trúc YOLO của bản trước (YOLOv8), nhóm tác giả thực hiện hiệu chỉnh các nội\n",
            "dung sau:\n",
            "-Lightweight classification head: Nhóm tác giả thực hiện giảm bớt độ lớn về kích\n",
            "thước của nhánh Classification khi nhận thấy với cùng một kiến trúc, nhánh Regres-\n",
            "sion cho thấy mức độ ảnh hưởng lớn đến hiệu suất của YOLO hơn. Vì vậy, tác giả sử\n",
            "dụng hai layer depth-wise convolution với kernel 3x3 đi kèm với point-wise convolution\n",
            "với kernel 1x1. Điều này sẽ làm giảm đáng kể số lượng tham số của kiến trúc mô hình\n",
            "cũng như thời gian xử lý.\n",
            "-Spatial-channel decoupled downsampling: Các mạng YOLO thường sử dụng\n",
            "layer convolution với kernel 3x3 và stride=2 để giảm kích thước feature map xuống.\n",
            "Điều này được nhóm tác giả quan sát cho thấy chi phí tính toán còn lớn. Vì vậy, tương\n",
            "tự như với classification head, YOLOv10 cũng sử dụng kết hợp phép point-wise và\n",
            "depth-wise convolution để thay thế phương thức thông thường.\n",
            "10' metadata={'source': 'downloaded_file.pdf', 'page': 9}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Hình 12: Minh họa về phép depth-wise convolution và point-wise convolution để thay thế phép\n",
            "convolution thông thường. Ảnh: link.\n",
            "-Rank-guided block design: Quan sát YOLOv8, tác giả nhận thấy toàn bộ các\n",
            "stage trong kiến trúc đều sử dụng chung một building block. Tuy nhiên, thông qua\n",
            "việc tính intrinsic rank của mỗi stage, tác giả chỉ ra rằng ở các stage gần cuối có sự\n",
            "dư thừa (redundancy) về mặt tham số lớn, dẫn đến sự không tối ưu về chi phí tính\n",
            "toán và lưu trữ. Vì vậy, để khắc phục, nhóm tác giả áp dụng chiến lược: duyệt qua\n",
            "các stage trong một mô hình theo thứ tự tăng dần về intrinsic rank, thực hiện thay\n",
            "thế basic block bằng một block được đề xuất là Compact Inverted Block (CIB). Các\n",
            "bạn có thể quan sát thành phần của block này ở ảnh dưới đây:\n",
            "Hình 13: Minh họa cấu trúc của Compact Inverted Block (CIB). Ảnh: [10].\n",
            "11' metadata={'source': 'downloaded_file.pdf', 'page': 10}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Về độ chính xác, dựa trên ý tưởng liên quan đến receptive field và phép self-attention,\n",
            "nhóm tác giả thực hiện hiệu chỉnh các nội dung sau:\n",
            "-Large-kernel convolution: Ở các phiên bản YOLOv10 kích thước nhỏ, nhóm tác\n",
            "giả thực hiện tăng kích thước kernel từ 3x3 lên 7x7 của phép depth-wise convolution\n",
            "trong CIB nhằm cải thiện receptive field. Việc gia tăng này chỉ được áp dụng ở các\n",
            "stage cuối.\n",
            "-Partial self-attention (PSA): Để tận dụng sức mạnh của phép self-attention. Kể từ\n",
            "sau stage 4 của mô hình, nhóm tác giả tách features sau phép point-wise convolution\n",
            "của CIB làm hai phần. Một phần sẽ được đẩy vào NPSAblock bao gồm sự kết hợp của\n",
            "lớp Multi-head self-attention (MHSA) và Feed-forward network (FFN), khá giống với\n",
            "Transformer Encoder. Phần LayerNorm sẽ được thay thế bằng BatchNorm để tăng\n",
            "tốc độ xử lý. Sau đó, kết quả của bước này sẽ được kết hợp với phần tách còn lại bởi\n",
            "phép point-wise convolution.' metadata={'source': 'downloaded_file.pdf', 'page': 11}\n",
            "page_content='phép point-wise convolution.\n",
            "Hình 14: Minh họa Partial self-attention (PSA). Ảnh: [10].\n",
            "12' metadata={'source': 'downloaded_file.pdf', 'page': 11}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "IV. Cài đặt chương trình và đánh giá\n",
            "Trong phần này, nhóm sẽ trình bày cách cài đặt, sử dụng và huấn luyện YOLOv10 trên bộ dữ\n",
            "liệu mới. Đồng thời, nhóm cũng thực hiện một thực nghiệm nhỏ nhằm so sánh hiệu suất của\n",
            "YOLOv10 so với hai phiên bản gần nhất là YOLOv8 và YOLOv9. Môi trường lập trình nhóm\n",
            "sử dụng là Google Colab.\n",
            "IV.I. Cài đặt và sử dụng pre-trained model\n",
            "Một cách nhanh chóng để sử dụng được YOLOv10 đó là sử dụng pre-trained model (mô hình đã\n",
            "được huấn luyện sẵn trên bộ dữ liệu COCO - một bộ dữ liệu rất lớn). Để sử dụng pre-trained\n",
            "model, các bạn làm như sau:\n",
            "1.Cài đặt các thư viện cần thiết: Tải về mã nguồn của YOLOv10 và cài đặt các thư\n",
            "viện trong file requirements.txt bằng các chạy đoạn code sau:\n",
            "1!git clone https :// github .com/THU -MIG/ yolov10 .git\n",
            "2%cd yolov10\n",
            "3!pip install -q -r requirements .txt\n",
            "4!pip install -e .\n",
            "2.Tải trọng số của pre-trained models: Để sử dụng được pre-trained models, chúng ta' metadata={'source': 'downloaded_file.pdf', 'page': 12}\n",
            "page_content='2.Tải trọng số của pre-trained models: Để sử dụng được pre-trained models, chúng ta\n",
            "cần tải về file trọng số (file .pt). Các bạn chạy đoạn code sau để tải về file trọng số phiên\n",
            "bản YOLOv10n:\n",
            "1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\n",
            "yolov10n .pt\n",
            "3.Khởi tạo mô hình: Để khởi tạo mô hình với trọng số vừa tải về, các bạn chạy đoạn code\n",
            "sau:\n",
            "1from ultralytics import YOLOv10\n",
            "2\n",
            "3model = YOLOv10 (\" yolov10n .pt\")\n",
            "4.Tải ảnh cần dự đoán: Chúng ta sẽ test mô hình trên một ảnh bất kì. Các bạn có thể tự\n",
            "chọn ảnh của riêng mình hoặc sử dụng ảnh tại đây. Các bạn có thể chạy đoạn code sau để\n",
            "tải ảnh này vào colab tự động:\n",
            "1! gdown \"1 tr9PSRRdlC2pNir7jsYugpSMG -7 v32VJ \" -O \"./ images /\"\n",
            "13' metadata={'source': 'downloaded_file.pdf', 'page': 12}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "5.Dự đoán: Để chạy dự đoán cho ảnh đã tải về, các bạn truyền đường dẫn ảnh vào mô hình\n",
            "như đoạn code sau:\n",
            "1image_path = \"./ images / HCMC_Street .jpg\"\n",
            "2result = model ( source = image_path )[0]\n",
            "Hình 15: Ảnh cần dự đoán.\n",
            "6.Lưu kết quả dự đoán: Để lưu lại ảnh đã được dự đoán, các bạn chạy đoạn code sau:\n",
            "1result . save (\"./ images / HCMC_Street_predict .png\")\n",
            "Hình 16: Kết quả dự đoạn của mô hình YOLOv10 phiên bản nano (yolov10n.pt) .\n",
            "7.Dự đoán youtube video: Để dự đoán với input là youtube video, các bạn chỉ cần thay\n",
            "thếimage_path bằng đường dẫn youtube video như đoạn code sau:\n",
            "14' metadata={'source': 'downloaded_file.pdf', 'page': 13}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "1youtube_video_path = \" https :// youtu .be/ wqPSsu7XQ74 \"\n",
            "2video_result = model ( source = youtube_video_path )\n",
            "Kết quả dự đoán sẽ là một video được lưu dưới dạng .avi trong thư mục: /content/yolov10\n",
            "/runs/detect/predict\n",
            "IV.II. Huấn luyện YOLOv10 trên tập dữ liệu mới\n",
            "Trong phần này, chúng ta sẽ thực hiện huấn luyện mô hình YOLOv10 (fine-tuning) trên một bộ\n",
            "dữ liệu với các class mới. Để tránh sự nhầm lẫn, phần này sẽ được thực hiện ở một file colab\n",
            "khác so với phần trước. Các bước thực hiện như sau:\n",
            "1.Tải bộ dữ liệu: Chúng ta sẽ giải quyết bài toán phát hiện các loại lá được phân biệt theo\n",
            "trình trạng bệnh của chúng. Bộ dữ liệu được sử dụng trong bài toán này là PlantDoc [12].\n",
            "Để dễ hình dung, các bạn có thể quan sát ảnh minh họa sau:\n",
            "Hình 17: Một vài mẫu dữ liệu trong bộ dữ liệu về bệnh của lá.\n",
            "Để tải bộ dữ liệu trên, các bạn hãy chạy đoạn code sau:\n",
            "1! gdown \"1 LBpKKXFcfvUVgyk3tgQH6YxNp1KXX0Va \"' metadata={'source': 'downloaded_file.pdf', 'page': 14}\n",
            "page_content='1! gdown \"1 LBpKKXFcfvUVgyk3tgQH6YxNp1KXX0Va \"\n",
            "Giải nén bộ dữ liệu vào folder datasets và xóa file nén không còn dùng đến. Các bạn thực\n",
            "thi đoạn code sau:\n",
            "15' metadata={'source': 'downloaded_file.pdf', 'page': 14}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "1! mkdir datasets\n",
            "2! unzip -q \"/ content / PlantDocv4 . zip\" -d \"/ content / datasets / PlantDocv4\n",
            "/\"\n",
            "3!rm / content / PlantDocv4 .zip\n",
            "Quan sát thư mục giải nén, có thể thấy bộ dữ liệu này đã được gán nhãn và đưa vào format\n",
            "cấu trúc dữ liệu training theo yêu cầu của YOLO. Vì vậy, chúng ta sẽ không cần thực hiện\n",
            "bước chuẩn bị dữ liệu ở bài này.\n",
            "2.Cài đặt và import các thư viện cần thiết: Tương tự như phần trước, các bạn chạy\n",
            "các đoạn code sau để cài đặt các gói thư viện để sử dụng được YOLOv10:\n",
            "1!git clone https :// github .com/THU -MIG/ yolov10 .git\n",
            "2%cd yolov10\n",
            "3!pip install -q -r requirements .txt\n",
            "4!pip install -e .\n",
            "3.Khởi tạo mô hình YOLOv10: Chúng ta sẽ khởi tạo mộ hình YOLOv10 với phiên\n",
            "bảnnano (n) từ trọng số đã được huấn luyện trên bộ dữ liệu COCO. Để tải trọng số\n",
            "yolov10n.pt, các bạn chạy đoạn code sau:\n",
            "1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\n",
            "yolov10n .pt' metadata={'source': 'downloaded_file.pdf', 'page': 15}\n",
            "page_content='1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\n",
            "yolov10n .pt\n",
            "Sau đó, để khởi tạo mô hình từ trọng số đã tải về, các bạn chạy đoạn code sau:\n",
            "1from ultralytics import YOLOv10\n",
            "2\n",
            "3model = YOLOv10 (\" yolov10n .pt\")\n",
            "4.Huấnluyệnmôhình: ChúngtatiếnhànhhuấnluyệnYOLOv10trênbộdữliệuPlantDoc\n",
            "với 100 epochs và kích thước ảnh là 640. Các bạn chạy đoạn code sau:\n",
            "1model . train ( data =\"../ datasets / PlantDocv4 / data . yaml \",\n",
            "2 epochs =100 ,\n",
            "3 imgsz =640)\n",
            "16' metadata={'source': 'downloaded_file.pdf', 'page': 15}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "Hình 18: Quá trình huấn luyện mô mình YOLOv10 trên tập dữ liệu PlantDoc.\n",
            "5.Đánh giá mô hình: Để thực hiện đánh giá mô hình trên tập test, các bạn chạy đoạn code\n",
            "sau:\n",
            "1model = YOLOv10 (\"./ runs / detect / train / weights / best .pt\")\n",
            "2\n",
            "3model .val ( data =\" ../ datasets / PlantDocv4 / data . yaml \",\n",
            "4 imgsz =640 ,\n",
            "5 split =\" test \")\n",
            "Hình 19: Đánh giá mô hình sau khi huấn luyện trên tập test.\n",
            "17' metadata={'source': 'downloaded_file.pdf', 'page': 16}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "IV.III. Đánh giá\n",
            "Nhóm thực hiện đánh giá mô hình YOLO qua các phiên bản v8, v9 và v10. Bằng cách lựa chọn\n",
            "cả 3 phiên bản có cùng số lượng tham số khoảng 25M tương ứng là YOLOv8-M, YOLOv9-C và\n",
            "YOLOv10-L. Các thử nghiệm được thực hiện trên cùng một thiết bị, python version, random\n",
            "seed và một số hyperparameter như: batch_size=16, image_size=640,... Sau khi quá trình huấn\n",
            "luyện kết thúc, thực hiện đánh giá trên tập test và ghi lại kết quả vào bảng 1 dưới đây:\n",
            "Bảng 1: Bảng thực nghiệm kết quả trên tập test của các mô hình YOLO phiên bản YOLOv8-M,\n",
            "YOLOv9-C, YOLOv10-M, YOLOv10-L sau khi fine-tuning.\n",
            "Model params GFLOPs layersinference time mAP@50 mAP@50-95\n",
            "YOLOv8-M 25.9M 78.9 295 6.5 ms 0.614 0.476\n",
            "YOLOv9-C 25.3M 102.1 618 8.1 ms 0.653 0.503\n",
            "YOLOv10-M 16.5 M 64.5 498 5.5 ms 0.626 0.479\n",
            "YOLOv10-L 24.4M 120.3 628 7.8 ms 0.659 0.498\n",
            "Quan sát kết quả trên, ta có thể thấy trên cùng một phiên bản, YOLOv10 có mức độ tối ưu tốt' metadata={'source': 'downloaded_file.pdf', 'page': 17}\n",
            "page_content='Quan sát kết quả trên, ta có thể thấy trên cùng một phiên bản, YOLOv10 có mức độ tối ưu tốt\n",
            "hơn về mặt tham số mô hình cũng như độ trễ trong inference trong khi vẫn giữ được độ chính\n",
            "xác ngang hoặc hơn so với các phiên bản trước.\n",
            "18' metadata={'source': 'downloaded_file.pdf', 'page': 17}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "V. Trích dẫn\n",
            "[1] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2015). You Only Look Once: Unified,\n",
            "Real-Time Object Detection. ArXiv. /abs/1506.02640\n",
            "[2] Redmon, J., & Farhadi, A. (2016). YOLO9000: Better, Faster, Stronger. ArXiv.\n",
            "/abs/1612.08242\n",
            "[3] Redmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. ArXiv.\n",
            "/abs/1804.02767\n",
            "[4] Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). YOLOv4: Optimal Speed and\n",
            "Accuracy of Object Detection. ArXiv. /abs/2004.10934\n",
            "[5] Jocher, G. (2020). YOLOv5 by Ultralytics. Zenodo. /record/3908559\n",
            "[6] Li, C., Li, L., Jiang, H., Weng, K., Geng, Y., & Wei, X. (2022). YOLOv6: A Single-Stage\n",
            "Object Detection Framework for Industrial Applications. ArXiv. /abs/2209.02976\n",
            "[7] Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2022). YOLOv7: Trainable bag-of-freebies\n",
            "sets new state-of-the-art for real-time object detectors. ArXiv. /abs/2207.02696' metadata={'source': 'downloaded_file.pdf', 'page': 18}\n",
            "page_content='sets new state-of-the-art for real-time object detectors. ArXiv. /abs/2207.02696\n",
            "[8] Jocher, G., Stoken, A., Borovec, J., Christopher, S. T. A. N., Laughing, L. C., & Ultralytics.\n",
            "(2023). YOLOv8 by Ultralytics. GitHub. /ultralytics/ultralytics\n",
            "[9] Wang, C., Yeh, I., & Liao, H. (2024). YOLOv9: Learning What You Want to Learn Using\n",
            "Programmable Gradient Information. ArXiv. /abs/2402.13616\n",
            "[10] Wang, A., Chen, H., Liu, L., Chen, K., Lin, Z., Han, J., & Ding, G. (2024). YOLOv10:\n",
            "Real-Time End-to-End Object Detection. ArXiv. /abs/2405.14458\n",
            "[11] Zhang, X., Wan, F., Liu, C., Ji, R., Ye, Q. (2019). FreeAnchor: Learning to Match Anchors\n",
            "for Visual Object Detection. ArXiv. /abs/1909.02466\n",
            "[12] Singh, Davinder and Jain, Naman and Jain, Pranjali and Kayal, Pratik and Kumawat,\n",
            "Sudhakar and Batra, Nipun (2020). PlantDoc: A Dataset for Visual Plant Disease Detection.\n",
            "https://doi.org/10.1145/3371158.3371196' metadata={'source': 'downloaded_file.pdf', 'page': 18}\n",
            "page_content='https://doi.org/10.1145/3371158.3371196\n",
            "[13] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C., & Berg, A. C. (2015). SSD:\n",
            "Single Shot MultiBox Detector. ArXiv. https://doi.org/10.1007/978-3-319-46448-0_2\n",
            "[14] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2013). Rich feature hierarchies for accu-\n",
            "rate object detection and semantic segmentation. ArXiv. /abs/1311.2524\n",
            "[15] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards Real-Time Object\n",
            "Detection with Region Proposal Networks. ArXiv. /abs/1506.01497\n",
            "[16] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020).\n",
            "End-to-End Object Detection with Transformers. ArXiv. /abs/2005.12872\n",
            "19' metadata={'source': 'downloaded_file.pdf', 'page': 18}\n",
            "page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\n",
            "[17] Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021).\n",
            "Emerging Properties in Self-Supervised Vision Transformers. ArXiv. /abs/2104.14294\n",
            "[18] Wang,J.,Song,L.,Li,Z.,Sun,H.,Sun,J.,&Zheng,N.(2020).End-to-EndObjectDetection\n",
            "with Fully Convolutional Network. ArXiv. /abs/2012.03544\n",
            "- Hết -\n",
            "20' metadata={'source': 'downloaded_file.pdf', 'page': 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "3fbhNe7CyCJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print (\" Number of sub-documents: \", len(docs))\n",
        "print (docs [0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1wQWQwA0Kyp",
        "outputId": "0fd748df-fa42-4ea1-e3d9-39602eb47cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Number of sub-documents:  33\n",
            "page_content='AI VIET NAM – AI COURSE 2024\n",
            "Tutorial: Phát hiện đối tượng trong ảnh với\n",
            "YOLOv10\n",
            "Dinh-Thang Duong, Nguyen-Thuan Duong, Minh-Duc Bui và\n",
            "Quang-Vinh Dinh\n",
            "Ngày 20 tháng 6 năm 2024\n",
            "I. Giới thiệu\n",
            "Object Detection (Tạm dịch: Phát hiện đối tượng) là một bài toán cổ điển thuộc lĩnh vực\n",
            "Computer Vision. Mục tiêu của bài toán này là tự động xác định vị trí của các đối tượng trong\n",
            "một tấm ảnh. Tính tới thời điểm hiện tại, đã có rất nhiều phương pháp được phát triển nhằm\n",
            "giải quyết hiệu quả bài toán này. Trong đó, các phương pháp thuộc họ YOLO (You Only Look\n",
            "Once) thu hút được sự chú ý rất lớn từ cộng đồng nghiên cứu bởi độ chính xác và tốc độ thực\n",
            "thi mà loại mô hình này mang lại.\n",
            "Hình 1: Logo của mô hình YOLO. Ảnh: link.\n",
            "Thời gian vừa qua, Ao Wang và các cộng sự tại Đại học Thanh Hoa (Tsinghua University)\n",
            "đã đề xuất mô hình YOLOv10 trong bài báo YOLOv10: Real-Time End-to-End Object\n",
            "Detection [10]. Với những cải tiến mới, mô hình đã đạt được hiệu suất vượt trội hơn so với các' metadata={'source': 'downloaded_file.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = HuggingFaceEmbeddings()\n",
        "vector_db = Chroma.from_documents(documents =docs, embedding = embedding)\n",
        "retriever = vector_db.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv84e9xF0jL8",
        "outputId": "541377df-46fc-4ac9-c08d-59713887e7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = retriever.invoke (\"YOLO là gì?\")\n",
        "\n",
        "print (\"Number of relevant documents: \", len(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSKrSv9y06_n",
        "outputId": "7a6e762b-72c1-4062-d394-891a3dd5f86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of relevant documents:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Machine training\n",
        "\n",
        "#1. Khai báo một số cài đặt cần thiết cho mô hình:\n",
        "\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "load_in_4bit = True ,\n",
        "bnb_4bit_quant_type = \"nf4\",\n",
        "bnb_4bit_use_double_quant = True,\n",
        "bnb_4bit_compute_dtype = torch.bfloat16)\n"
      ],
      "metadata": {
        "id": "V3T6p0cA1P1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Khởi tạo mô hình và tokenizer:\n",
        "MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained (\n",
        "MODEL_NAME,\n",
        "quantization_config = nf4_config,\n",
        "low_cpu_mem_usage = True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained (MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "724e70ceb031479d8c37b49868d9f8bc",
            "9e8915e1315344aabeba33240acc097f",
            "31f571b52b9144a5a21b1f64eef482e4",
            "afb00992b8f04c499490253cd7cb0a39",
            "bebf0cb48fac4dada89b10dfda03d306",
            "0fd1d4372eea4a02a75431359a7ed6f7",
            "97f2a64859de4ffa9da6be87c5419913",
            "7a3594931d614fab9ef0f79b1ffba318",
            "e8a2a957b6b940789d63dd155bd6f5b9",
            "71ec0f5cf74940aea617226ea978db4a",
            "5c03a5b1d6df4f0b91ea5ed0f36a658e"
          ]
        },
        "id": "jNMAyx4r1q3m",
        "outputId": "fab775d1-5089-43ba-cd0b-7d9305b3acae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "724e70ceb031479d8c37b49868d9f8bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Tích hợp tokenizer và model thành một pipeline để tiện sử dụng:\n",
        "model_pipeline = pipeline(\n",
        "\"text-generation\",\n",
        "model = model,\n",
        "tokenizer = tokenizer,\n",
        "max_new_tokens =512,\n",
        "pad_token_id = tokenizer.eos_token_id,\n",
        "device_map =\"auto\"\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline (pipeline = model_pipeline, )"
      ],
      "metadata": {
        "id": "SOy8ksIo52sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Chạy chương trình\n",
        "prompt = hub.pull (\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join (doc.page_content for doc in docs)\n",
        "\n",
        "# Assuming the necessary imports and configurations are done above\n",
        "\n",
        "# Define the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "USER_QUESTION = \"what is YOLOv10?\"\n",
        "output = rag_chain.invoke(USER_QUESTION)\n",
        "\n",
        "# Adding error handling for the split operation\n",
        "try:\n",
        "    answer = output.split('Answer :')[1].strip()\n",
        "except IndexError:\n",
        "    answer = \"Could not find the answer in the output.\"\n",
        "\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXiHQcS06amG",
        "outputId": "95b35510-23c7-467c-aeef-d6871bab6792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find the answer in the output.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Chainlit\n",
        "! pip install -q transformers==4.41.2\n",
        "! pip install -q bitsandbytes==0.43.1\n",
        "! pip install -q accelerate==0.31.0\n",
        "! pip install -q langchain==0.2.5\n",
        "! pip install -q langchainhub==0.1.20\n",
        "! pip install -q langchain-chroma==0.1.1\n",
        "! pip install -q langchain-community==0.2.5\n",
        "! pip install -q langchain-openai==0.1.9\n",
        "! pip install -q langchain_huggingface==0.0.3\n",
        "! pip install -q chainlit==1.1.304\n",
        "! pip install -q python-dotenv ==1.0.1\n",
        "! pip install -q pypdf==4.2.0\n",
        "! npm install -g localtunnel\n",
        "! pip install -q numpy==1.25.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFBzh4R7BHTP",
        "outputId": "d6decc09-f3f4-4610-ac35-7f7eb378a5d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Invalid requirement: '==1.0.1'\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors in 1.831s\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chainlit 1.1.304 requires numpy<2.0,>=1.26; python_version >= \"3.9\", but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chainlit as cl\n",
        "import torch\n",
        "\n",
        "from chainlit.types import AskFileResponse\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter (chunk_size =1000, chunk_overlap =100)\n",
        "embedding = HuggingFaceEmbeddings()\n",
        "\n",
        "def process_file (file: AskFileResponse) :\n",
        "if file.type == \" text/plain \":\n",
        "Loader = TextLoader\n",
        "elif file . type == \" application / pdf \":\n",
        "Loader = PyPDFLoader\n",
        "\n",
        "loader = Loader (file . path)\n",
        "documents = loader.load ()\n",
        "docs = text_splitter.split_documents (documents)\n",
        "for i, doc in enumerate (docs) :\n",
        "doc.metadata [\" source \"] = f\" source_ {i}\"\n",
        "return docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ZwWCJywrCHcf",
        "outputId": "b0c844a3-d499-45d7-8d08-829d402404a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 25 (<ipython-input-5-d32a1afc1347>, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-d32a1afc1347>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    if file.type == \" text/plain \":\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_db (file:AskFileResponse) :\n",
        "    docs = process_file (file)\n",
        "    cl. user_session.set (\"docs\", docs)\n",
        "    vector_db = Chroma.from_documents (documents =docs ,\n",
        "    embedding = embedding)\n",
        "    return vector_db"
      ],
      "metadata": {
        "id": "ZKu81BsuHDLq"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}